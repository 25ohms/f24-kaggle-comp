{"cells":[{"cell_type":"markdown","metadata":{"id":"1fjtH2YCI_Kp"},"source":["Importing modules + training data"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":876,"status":"ok","timestamp":1731990779041,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"sQxpvQgmIYLp"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn import preprocessing, linear_model, model_selection, metrics, neighbors, ensemble\n","import numpy as np\n","\n","# your files might be in \"Shared with me\" instead of \"My Drive\", or you could just manually upload to the Colab and remove the \"drive/My Drive/STAT441 Kaggle/\" part entirely\n","df_ed_train = pd.read_csv(\"./module_Education_train_set.csv\")\n","df_hh_train = pd.read_csv(\"./module_HouseholdInfo_train_set.csv\")\n","y_train = pd.read_csv(\"./module_SubjectivePoverty_train_set.csv\")\n","\n","df_ed_test = pd.read_csv(\"./module_Education_test_set.csv\")\n","df_hh_test = pd.read_csv(\"./module_HouseholdInfo_test_set.csv\")"]},{"cell_type":"markdown","metadata":{"id":"dDc3XWC_JPXa"},"source":["Combining DFs"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3270,"status":"ok","timestamp":1731990783948,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"8E5bpnR2JZRW","outputId":"ad5c07ac-d376-414f-c1c6-6080f0a3019d"},"outputs":[],"source":["#adding the 'psu_hh_idcode' column for future merging\n","df_ed_train['psu_hh_idcode'] = ['_'.join([str(int(row['psu'])),str(int(row['hh'])),str(int(row['idcode']))]) for index, row in df_ed_train.iterrows()]\n","df_ed_train.drop(['hh', 'idcode'], axis=1, inplace=True)\n","\n","df_hh_train['psu_hh_idcode'] = ['_'.join([str(int(row['psu'])),str(int(row['hh'])),str(int(row['idcode']))]) for index, row in df_hh_train.iterrows()]\n","df_hh_train.drop(['hh', 'idcode'], axis=1, inplace=True)\n","\n","df_ed_test['psu_hh_idcode'] = ['_'.join([str(int(row['psu'])),str(int(row['hh'])),str(int(row['idcode']))]) for index, row in df_ed_test.iterrows()]\n","df_ed_test.drop(['hh', 'idcode'], axis=1, inplace=True)\n","\n","\n","df_hh_test['psu_hh_idcode'] = ['_'.join([str(int(row['psu'])),str(int(row['hh'])),str(int(row['idcode']))]) for index, row in df_hh_test.iterrows()]\n","df_hh_test.drop(['hh', 'idcode'], axis=1, inplace=True)\n","\n","# combining the subjective_poverty_n columns into a single column\n","y_train = y_train[y_train['psu_hh_idcode'].isin(df_ed_train['psu_hh_idcode'])]\n","sp_onehot = y_train.drop(columns=['psu_hh_idcode'])\n","y_train['subjective_poverty'] = pd.from_dummies(sp_onehot)\n","y_train['subjective_poverty'] = [int(x[19:]) for x in y_train['subjective_poverty']]\n","y_train.drop(columns=['subjective_poverty_' + str(i) for i in range(1,11)], inplace=True)\n","\n","df_ed_train.set_index('psu_hh_idcode', inplace=True)\n","df_hh_train.set_index('psu_hh_idcode', inplace=True)\n","df_ed_test.set_index('psu_hh_idcode', inplace=True)\n","df_hh_test.set_index('psu_hh_idcode', inplace=True)\n","\n","y_train.set_index('psu_hh_idcode', inplace=True)\n","\n","#MAYBE REMOVE THIS PART:\n","# All other columns in the education test set are almost 99% NA\n","# Theoretically including all the rest of the columns should only give at most a score improvement of ~0.023\n","# since it only applies to 1% of the training data (and just giving every option a 0.1 chance is a log loss of 2.3)\n","##### df_ed_train = df_ed_train[['q01', 'q02', 'q03', 'q04', 'q05', 'q06', 'q07', 'Q08', 'Q11', 'Q14', 'Q17', 'Q18', 'Q19']]\n","df_ed_train['set'] = 'train'\n","##### df_ed_test = df_ed_test[['q01', 'q02', 'q03', 'q04', 'q05', 'q06', 'q07', 'Q08', 'Q11', 'Q14', 'Q17', 'Q18', 'Q19']]\n","df_ed_test['set'] = 'test'\n","\n","# combining training and test sets\n","df_ed = pd.concat([df_ed_train, df_ed_test])\n","df_hh = pd.concat([df_hh_train, df_hh_test])\n","\n","df = df_hh.join(df_ed, lsuffix=\"_hh\", how='inner')\n"]},{"cell_type":"markdown","metadata":{"id":"F_cmTwOwJ5O1"},"source":["Filling in NAs"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6130,"status":"ok","timestamp":1731982494846,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"tktaSGDDJ85_"},"outputs":[],"source":["###df['q07'].fillna(0) # q07 in education is NA => never went to school => 0 years of preschool\n","\n","\n","def highest_education(index):\n","  row = df_ed.loc[index]\n","  if row['q03'] == 2 or row['q06'] == 0:\n","    return 1 # no education\n","  elif row['q06'] == 1:\n","    return 2 # primary 4/5\n","  elif row['q06'] == 2 and row['q04'] == 1:\n","    return 3 # primary 7/8/9\n","  elif row['q04'] == 2 and row['q06'] < 3:\n","    return 4 # some secondary\n","  elif row['q06'] == 3 and row['q04'] == 2:\n","    return 5 # finished secondary\n","  elif row['q06'] < 4 and row['q04'] in [3,4,5]:\n","    return 6 # some vocational (pretty sure \"Technicum\" is  a type of vocational)\n","  elif row['q06'] in [4,5,6] and row['q04'] < 6:\n","    return 7 # finished vocational\n","  elif row['q06'] < 7 and row['q04'] >= 6:\n","    return 8 # some university\n","  elif row['q06'] in [7,8,9]:\n","    return 9 # finished university\n","  else:\n","    return 10 # Post university\n","\n","\n","\n","for index, row in df.iterrows():\n","  if row['q11'] == 1:\n","    mother = '_'.join(index.split('_')[:-1] + [str(int(row['q12']))])\n","    if mother in df_ed.index:\n","        row['q13'] = highest_education(mother)\n","    elif mother in df_ed.index:\n","        row['q13'] = highest_education(mother)\n","    row['q14'] = 1\n","    if mother in df_hh.index:\n","        row['q16'] = df_hh.loc[mother]['q05y']\n","\n","  if row['q17'] == 1:\n","    father = '_'.join(index.split('_')[:-1] + [str(int(row['q18']))])\n","    if father in df_ed.index:\n","        row['q19'] = highest_education(father)\n","    row['q20'] = 1\n","    if father in df_hh.index:\n","        row['q22'] = df_hh.loc[father]['q05y']\n","\n","df.drop(columns=['q12', 'q18'], inplace=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["Comparing distributions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# First analyze NA distributions across all columns\n","print(\"Initial NA Percentages:\")\n","for col in df.columns:\n","    if col not in ['set', 'subjective_poverty']:  # Skip these metadata columns\n","        train_na = df[df['set'] == 'train'][col].isna().mean()\n","        test_na = df[df['set'] == 'test'][col].isna().mean()\n","        \n","        # Only print if there are any NAs in either set\n","        if train_na > 0 or test_na > 0:\n","            print(f\"\\n{col}:\")\n","            print(f\"Train NA%: {train_na:.2%}\")\n","            print(f\"Test NA%: {test_na:.2%}\")\n","            \n","            # Also show value distribution for columns with NAs\n","            print(\"\\nValue distribution (excluding NAs):\")\n","            print(df[col].value_counts(normalize=True).head())\n"]},{"cell_type":"markdown","metadata":{},"source":["sorting differences"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate NA percentage differences between train and test sets\n","na_differences = {}\n","for col in df.columns:\n","    if col not in ['set', 'subjective_poverty']:\n","        train_na = df[df['set'] == 'train'][col].isna().mean()\n","        test_na = df[df['set'] == 'test'][col].isna().mean()\n","        difference = abs(train_na - test_na)\n","        \n","        if difference > 0:  # Only store columns with differences\n","            na_differences[col] = {\n","                'difference': difference,\n","                'train_na': train_na,\n","                'test_na': test_na\n","            }\n","\n","# Sort and display columns by discrepancy\n","sorted_differences = sorted(na_differences.items(), \n","                          key=lambda x: x[1]['difference'], \n","                          reverse=True)\n","\n","print(\"NA Percentage Discrepancies (Train vs Test):\")\n","print(\"\\nColumn | Train NA% | Test NA% | Difference\")\n","print(\"-\" * 50)\n","for col, stats in sorted_differences:\n","    print(f\"{col:15} | {stats['train_na']:8.2%} | {stats['test_na']:7.2%} | {stats['difference']:.2%}\")"]},{"cell_type":"markdown","metadata":{"id":"RLf0sCIh4Khs"},"source":["Feature engineering"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"elapsed":9176,"status":"ok","timestamp":1731982504020,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"hrfiN4ZF4NfK","outputId":"5180ee42-3874-4803-803b-8b9d037b0911"},"outputs":[],"source":["#df['highest_ed'] = [highest_education(ind) for ind, row in df.iterrows()]\n","#df.drop(columns=['q04', 'q05'], inplace=True)\n","\n","# Replace Q05\n","df['years_primary'] = [row['q05'] if row['q04'] == 1 else 0 for i,row in df.iterrows()]\n","df['years_secondary'] = [row['q05'] if row['q04'] == 2 else 0 for i,row in df.iterrows()]\n","df['years_vocational'] = [row['q05'] if row['q04'] in [3,4,5] else 0 for i,row in df.iterrows()]\n","df['years_uni'] = [row['q05'] if row['q04'] in [6,7] else 0 for i,row in df.iterrows()]\n","df['years_postgrad'] = [row['q05'] if row['q04'] in [8,9,10,11] else 0 for i,row in df.iterrows()]\n","df.drop(columns=['q05'], inplace=True)\n","#df.drop(columns=['q05', 'Q13', 'Q22'], inplace=True)\n","\n","# Combine postsecondary for q4\n","#df['q04'] = [min(row['q04'], 6.0) for i,row in df.iterrows()]\n","# ^ TRY WITHOUT THIS ERR(1)\n","\n","# combine age (year and month) into single number\n","# Note: Maybe change this to categorical for logistic regression? Seems unlikely\n","#       to be linear\n","df['age'] = df['q05y'] + df['q05m'] / 12\n","df.drop(columns=['q05y', 'q05m'], inplace=True)\n","def age_group(age):\n","    if (age < 12):\n","        return 0\n","    elif (age < 18):\n","        return 1\n","    elif (age < 30):\n","        return 2\n","    elif (age < 40):\n","        return 3\n","    elif (age < 50):\n","        return 4\n","    elif (age < 60):\n","        return 5\n","    else:\n","        return 6\n","df['age_group'] = [age_group(age) for age in df['age']]\n","#df.drop(columns=['age'], inplace=True)\n","\n","print(df['age_group'].value_counts())\n","\n","# following commented out because we removed the q24 column\n","\n","\"\"\"\n","# turn q24_ed (distance from school) into categorical\n","def distance_category(dist):\n","  if dist == 0:\n","    return 0\n","  if dist < 1:\n","    return 1\n","  if dist < 2:\n","    return 2\n","  if dist < 3:\n","    return 3\n","  if dist < 4:\n","    return 4\n","  if dist < 5:\n","    return 5\n","  if dist < 10:\n","    return 6\n","  if dist < 50:\n","    return 7\n","  if dist < 100:\n","    return 8\n","  return 9\n","\n","df['dist_from_school'] = [distance_category(dist) for dist in df['Q24']]\n","df.drop(columns=['Q24'], inplace=True)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"cQf_B1MtddVv"},"source":["One hot encoding for categorical variables + cleanup"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1731982504247,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"motctahedjKZ"},"outputs":[],"source":["categorical = ['q01', 'q02', 'q06','Q11', 'Q17','q03_hh', 'q06_hh', 'q13', 'q19',\n","               'age_group', 'psu']#'highest_ed']\n","\n","# following commented out because we removed most columns\n","\"\"\"categorical = ['q04', 'q06', 'Q10', 'Q11', 'Q12', 'Q16', 'Q17',\n","               'Q21', 'Q23', 'Q28', 'Q42', 'Q44', 'Q48', 'Q52',\n","               'Q53', 'Q63', 'Q66', 'q03_hh', 'q06_hh', 'q13', 'q19',\n","               'dist_from_school']\n","\"\"\"\n","\n","# create onehot df and add it to the original df\n","for col in categorical:\n","    new_onehot = pd.get_dummies(df[col], prefix = col)\n","    df = pd.concat([df, new_onehot],axis=1)\n","\n","# drop the original categorical\n","df.drop(columns=categorical, inplace=True)\n","\n","# useless columns (q04_hh is date of birth, redundant with q05 which is age)\n","# hhid would be useful, but no observations in the train and test set share a hhid\n","df.drop(columns=['q04_hh', 'hhid'], inplace=True)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# making parent age death categorical\n","\n","# First modify the parent age categorization to use string labels instead of numbers\n","def categorize_parent_age(is_alive, age):\n","    if pd.isna(age):\n","        if is_alive == 1:  \n","            return 'alive'\n","        else:  \n","            return 'deceased_unknown'\n","    else:  \n","        if age < 40:\n","            return 'died_under_40'\n","        elif age < 50:\n","            return 'died_40_50'\n","        elif age < 60:\n","            return 'died_50_60'\n","        elif age < 70:\n","            return 'died_60_70'\n","        else:\n","            return 'died_over_70'\n","\n","# Apply categorization\n","df['mother_status'] = df.apply(\n","    lambda row: categorize_parent_age(\n","        row['q14'] if 'q14' in row else row['q11'],\n","        row['q15']\n","    ), axis=1\n",")\n","\n","df['father_status'] = df.apply(\n","    lambda row: categorize_parent_age(\n","        row['q20'] if 'q20' in row else row['q17'],\n","        row['q21']\n","    ), axis=1\n",")\n","\n","# One-hot encode the new categorical columns\n","mother_dummies = pd.get_dummies(df['mother_status'], prefix='mother')\n","father_dummies = pd.get_dummies(df['father_status'], prefix='father')\n","\n","# Add the dummy columns to the dataframe and drop the original columns\n","df = pd.concat([df, mother_dummies, father_dummies], axis=1)\n","df.drop(columns=['mother_status', 'father_status', 'q15', 'q21'], inplace=True)\n","\n","# Print value distributions to verify\n","#print(\"\\nMother age category distribution:\")\n","#print(df['q15'].value_counts(normalize=True))\n","#print(\"\\nFather age category distribution:\")\n","#print(df['q21'].value_counts(normalize=True))"]},{"cell_type":"markdown","metadata":{"id":"eax9IGzkduG2"},"source":["Normalization + train test split + filling in remaining NAs"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# For real test set:\n","#df_train = df.join(y_train, how='inner')\n","#df_test = df[df['set'] == 'test']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2391,"status":"ok","timestamp":1731982506637,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"Btbnh2lzdwKe","outputId":"761717db-4dba-4f72-cbc4-8748ab996a6d"},"outputs":[],"source":["\n","#----------------------------------------------------------------------------------------------------------------------\n","# For development test set:\n","df_final = df.join(y_train, how='inner')\n","df_train,df_test = model_selection.train_test_split(df_final, train_size = 0.8)\n","#----------------------------------------------------------------------------------------------------------------------\n","\n","same_cols = []\n","\n","means = {}\n","stds = {}\n","\n","# standardization\n","for col in df_train:\n","    if col not in ['psu_hh_idcode', 'subjective_poverty', 'set'] :\n","        if df_train[col].std() == 0 or df_test[col].std() == 0:\n","            print(\"all same\", col)\n","            same_cols.append(col)\n","            continue\n","        means[col] = df_train[col].mean()\n","        stds[col] = df_train[col].std()\n","        df_train[col] = (df_train[col] - means[col]) / stds[col]\n","        df_test[col] = (df_test[col] - means[col]) / stds[col]\n","\n","# drop columns that are all the same (i.e. give no info) in train or test set\n","df_train.drop(columns=same_cols, inplace=True)\n","df_test.drop(columns=same_cols, inplace=True)\n","\n","# fill remaining NAs with 0 (i.e. mean)\n","# maybe change this part to be smarter\n","df_train.fillna(0, inplace=True)\n","df_test.fillna(0, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check for any remaining NA values\n","print(\"NA values in training set:\")\n","print(df_train.isna().sum().sum())\n","print(\"\\nNA values in test set:\") \n","print(df_test.isna().sum().sum())\n"]},{"cell_type":"markdown","metadata":{"id":"gz8e49eSeLfA"},"source":["Fit + eval for logistic regression, KNN, random forest, boosting"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1296,"status":"ok","timestamp":1731982507931,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"D2VCbqYteOIX"},"outputs":[],"source":["import numpy as np\n","import scipy.ndimage\n","\n","def smooth(p):\n","    # Ensure the probabilities sum to 1\n","    p = p / np.sum(p)\n","\n","    # Identify the index and value of the maximum probability\n","    max_idx = np.argmax(p)\n","    p_max = p[max_idx]\n","\n","    # Set K as (N / 2) squared, where N is the length of p\n","    N = len(p)\n","    K = (N / 2) ** 2\n","\n","    # Compute sigma_squared based on the maximum probability\n","    sigma_squared = K * (1 - p_max)\n","\n","    # If sigma_squared is zero, no smoothing is needed\n","    if sigma_squared == 0:\n","        return p  # Return the original probabilities\n","\n","    # Compute Gaussian weights centered at max_idx\n","    indices = np.arange(N)\n","    gaussian_weights = np.exp(-((indices - max_idx) ** 2) / (2 * sigma_squared))\n","\n","    # Apply the Gaussian weights to the probabilities\n","    p_new = p * gaussian_weights\n","\n","    # Normalize the new probabilities so they sum to 1\n","    p_new /= np.sum(p_new)\n","\n","    return p_new\n","\n","for idx, item in df_train.iterrows():\n","  if item[\"set\"] == \"test\":\n","    print(\"misrepresented\")\n","\n","X = df_train.drop(columns=[\"subjective_poverty\", \"set\"])\n","y = df_train[\"subjective_poverty\"]\n","\n","\n","#development test set\n","X_test = df_test.drop(columns=[\"subjective_poverty\", \"set\"])\n","y_test = df_test[\"subjective_poverty\"]\n","\n","# actual test set\n","#X_test = df_test.drop(columns=[\"set\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Number of NA values in X:\")\n","print(X.isna().sum().sum())\n","print(\"\\nNA values by column:\")\n","print(X.isna().sum())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":13319,"status":"ok","timestamp":1731982521249,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"njUmLcdeA6Fj"},"outputs":[],"source":["# First fit the model\n","lr = linear_model.LogisticRegressionCV(\n","    cv=5,\n","    penalty='l2',\n","    scoring='neg_log_loss',\n","    max_iter=1000,\n","    n_jobs=-1,\n","    random_state=42\n",")\n","lr.fit(X, y)\n","\n","# Get the mean scores and standard errors\n","mean_scores = -np.mean(lr.scores_[list(lr.scores_.keys())[0]], axis=0)  # Negative because we used neg_log_loss\n","std_scores = np.std(lr.scores_[list(lr.scores_.keys())[0]], axis=0)\n","\n","# Find the minimum score and its index\n","min_score_idx = np.argmin(mean_scores)\n","min_score = mean_scores[min_score_idx]\n","\n","# Find the largest C value whose score is within one std err of the minimum\n","threshold = min_score + std_scores[min_score_idx]\n","valid_indices = np.where(mean_scores <= threshold)[0]\n","one_se_idx = valid_indices[0]  # Take the largest C value (first index due to C being in descending order)\n","optimal_c = lr.C_[one_se_idx]\n","\n","# Refit with the one-SE C value\n","lr_final = linear_model.LogisticRegression(\n","    C=optimal_c,\n","    penalty='l2',\n","    max_iter=1000,\n","    random_state=42\n",")\n","\n","probs_lr = lr_final.fit(X,y).predict_proba(X_test)\n","probs_lr_smooth = [smooth(i) for i in probs_lr]\n","\n","\n","#development test set\n","#print(metrics.log_loss(y_test, probs_lr, labels = ['subjective_poverty_' + str(i) for i in range(1,11)]))\n","#print(metrics.log_loss(y_test, probs_lr_smooth, labels = ['subjective_poverty_' + str(i) for i in range(1,11)]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#development test set\n","print(metrics.log_loss(y_test, probs_lr, labels = [i for i in range(1,11)]))\n","print(metrics.log_loss(y_test, probs_lr_smooth, labels = [i for i in range(1,11)]))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1052,"status":"ok","timestamp":1731982609114,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"71eE7R2x8k9A"},"outputs":[],"source":["# Find optimal k using cross-validation\n","k_range = range(1, 501, 10)  # Test k values from 1 to 500 in steps of 10\n","cv_scores = []\n","\n","for k in k_range:\n","    knn = neighbors.KNeighborsClassifier(n_neighbors=k, weights='distance')\n","    scores = model_selection.cross_val_score(knn, X, y, cv=5, scoring='neg_log_loss')\n","    cv_scores.append(-scores.mean())  # Negative because we want to minimize log loss\n","\n","# Find the optimal k\n","optimal_k = k_range[np.argmin(cv_scores)]\n","\n","# Train final model with optimal k\n","knn = neighbors.KNeighborsClassifier(n_neighbors=optimal_k, weights='distance')\n","probs_knn = knn.fit(X,y).predict_proba(X_test)\n","\n","probs_knn_smooth = [smooth(i) for i in probs_knn]\n","\n","# Print results\n","print(f\"Optimal k: {optimal_k}\")\n","print(f\"Log loss (unsmoothed): {metrics.log_loss(y_test, probs_knn)}\")\n","print(f\"Log loss (smoothed): {metrics.log_loss(y_test, probs_knn_smooth)}\")\n","\n","# Optionally plot CV results\n","plt.figure(figsize=(10, 6))\n","plt.plot(k_range, cv_scores)\n","plt.xlabel('Number of Neighbors (k)')\n","plt.ylabel('Cross-Validation Log Loss')\n","plt.title('KNN Cross-Validation Results')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(metrics.log_loss(y_test, probs_knn, labels = [i for i in range(1,11)]))\n","print(metrics.log_loss(y_test, probs_knn_smooth, labels = [i for i in range(1,11)]))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7384,"status":"ok","timestamp":1731982616497,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"kDaa2KqxnkHt"},"outputs":[],"source":["rf = ensemble.RandomForestClassifier(random_state=42, criterion=\"log_loss\")\n","probs_rf = rf.fit(X,y).predict_proba(X_test)\n","probs_rf_smooth = [smooth(i) for i in probs_rf]\n","print(metrics.log_loss(y_test, probs_rf, labels = [i for i in range(1,11)]))\n","print(metrics.log_loss(y_test, probs_rf_smooth, labels = [i for i in range(1,11)]))"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":61205,"status":"ok","timestamp":1731982677700,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"Au59pf9_nlgq"},"outputs":[],"source":["gb = ensemble.GradientBoostingClassifier(random_state=42)\n","probs_gb = gb.fit(X,y).predict_proba(X_test)\n","probs_gb_smooth = [smooth(i) for i in probs_gb]\n","\n","# development test set\n","#print(metrics.log_loss(y_test, probs_gb, labels = ['subjective_poverty_' + str(i) for i in range(1,11)]))\n","#print(metrics.log_loss(y_test, probs_gb_smooth, labels = ['subjective_poverty_' + str(i) for i in range(1,11)]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# development test set\n","print(metrics.log_loss(y_test, probs_gb, labels = [i for i in range(1,11)]))\n","print(metrics.log_loss(y_test, probs_gb_smooth, labels = [i for i in range(1,11)]))"]},{"cell_type":"code","execution_count":284,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1731982677700,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"Wanv0BHQ-lKh"},"outputs":[],"source":["# Really simple one\n","overall_probs = y_train.value_counts() / sum(y_train.value_counts())\n","y_pred = pd.DataFrame(index=X_test.index, columns = ['subjective_poverty_' + str(i) for i in range(1,11)])\n","for i in range(1,11):\n","    y_pred['subjective_poverty_' + str(i)] = overall_probs['subjective_poverty_' + str(i)]\n","#print(metrics.log_loss(y_test, y_pred, labels = ['subjective_poverty_' + str(i) for i in range(1,11)]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":835244,"status":"ok","timestamp":1731983512937,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"FPnfYe3SBeTl","outputId":"df570e31-89e0-426c-bd8f-17d4a5988481"},"outputs":[],"source":["stack = ensemble.StackingClassifier([('logistic CV', lr_final), ('KNN', knn), (\"Gradient boosting\",gb)])\n","stack_fit = stack.fit(X,y)\n","probs_stack = stack_fit.predict_proba(X_test)\n","probs_stack_smooth = [smooth(i) for i in probs_stack]\n","\n","# development test set\n","scores = model_selection.cross_val_score(stack, X, y, cv=5, scoring='neg_log_loss', n_jobs=5)\n","print(f\"Scores: {-scores}, Mean scores: {-scores.mean()}\")\n","\n","#real test set\n","#out_df = pd.DataFrame(index=df_test.index, columns = ['subjective_poverty_' + str(i) for i in range(1,11)], data=probs_stack)\n","\n","#  print(len(out_df))\n","#out_df.to_csv(\"probs.csv\")"]},{"cell_type":"code","execution_count":286,"metadata":{"executionInfo":{"elapsed":150622,"status":"ok","timestamp":1731983663557,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"G8fp7osCBgtY"},"outputs":[],"source":["things = {i:stack_fit.predict_proba(X_test.loc[[i]]) for i,row in X_test.iterrows()}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":317,"status":"ok","timestamp":1731983663867,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"u9N_YFTEHpeg","outputId":"b657b638-54ac-40b1-c1e3-9835cf38085f"},"outputs":[],"source":["things2 = {i:things[i][0] for i in things}\n","\n","new_df = pd.DataFrame.from_dict(things2, orient='index', columns = ['subjective_poverty_' + str(i) for i in range(1,11)])\n","\n","m = 0\n","for i in new_df.index:\n","    m = max(m, sum(abs(new_df.loc[i] - out_df.loc[i])))\n","print(m)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1103},"executionInfo":{"elapsed":2430,"status":"ok","timestamp":1731987249970,"user":{"displayName":"Kevin Li","userId":"02362265452529969860"},"user_tz":300},"id":"LUjV1Z0rH1Yp","outputId":"484a3885-01d9-47c3-e579-9052d5dac143"},"outputs":[],"source":["#adding the 'psu_hh_idcode' column for future merging\n","df_ed_train['psu_hh_idcode'] = ['_'.join([str(int(row['psu'])),str(int(row['hh'])),str(int(row['idcode']))]) for index, row in df_ed_train.iterrows()]\n","df_ed_train.drop(['hh', 'idcode'], axis=1, inplace=True)\n","\n","df_hh_train['psu_hh_idcode'] = ['_'.join([str(int(row['psu'])),str(int(row['hh'])),str(int(row['idcode']))]) for index, row in df_hh_train.iterrows()]\n","df_hh_train.drop(['hh', 'idcode'], axis=1, inplace=True)\n","\n","df_ed_test['psu_hh_idcode'] = ['_'.join([str(int(row['psu'])),str(int(row['hh'])),str(int(row['idcode']))]) for index, row in df_ed_test.iterrows()]\n","df_ed_test.drop(['hh', 'idcode'], axis=1, inplace=True)\n","\n","df_hh_test['psu_hh_idcode'] = ['_'.join([str(int(row['psu'])),str(int(row['hh'])),str(int(row['idcode']))]) for index, row in df_hh_test.iterrows()]\n","df_hh_test.drop(['hh', 'idcode'], axis=1, inplace=True)\n","\n","# combining the subjective_poverty_n columns into a single column\n","y_train = y_train[y_train['psu_hh_idcode'].isin(df_ed_train['psu_hh_idcode'])]\n","sp_onehot = y_train.drop(columns=['psu_hh_idcode'])\n","y_train['subjective_poverty'] = pd.from_dummies(sp_onehot)\n","y_train.drop(columns=['subjective_poverty_' + str(i) for i in range(1,11)], inplace=True)\n","\n","df_ed_train.set_index('psu_hh_idcode', inplace=True)\n","df_hh_train.set_index('psu_hh_idcode', inplace=True)\n","df_ed_test.set_index('psu_hh_idcode', inplace=True)\n","df_hh_test.set_index('psu_hh_idcode', inplace=True)\n","\n","y_train.set_index('psu_hh_idcode', inplace=True)\n","\n","#MAYBE REMOVE THIS PART:\n","# All other columns in the education test set are almost 99% NA\n","# Theoretically including all the rest of the columns should only give at most a score improvement of ~0.023\n","# since it only applies to 1% of the training data (and just giving every option a 0.1 chance is a log loss of 2.3)\n","df_ed_train = df_ed_train[['q01', 'q02', 'q03', 'q04', 'q05', 'q06', 'q07', 'Q08', 'Q11', 'Q14', 'Q17', 'Q18', 'Q19']]\n","df_ed_train['set'] = 'train'\n","df_ed_test = df_ed_test[['q01', 'q02', 'q03', 'q04', 'q05', 'q06', 'q07', 'Q08', 'Q11', 'Q14', 'Q17', 'Q18', 'Q19']]\n","df_ed_test['set'] = 'test'\n","\n","# combining training and test sets\n","df_ed = pd.concat([df_ed_train, df_ed_test])\n","df_hh = pd.concat([df_hh_train, df_hh_test])\n","\n","df = df_hh.join(df_ed, lsuffix=\"_hh\", how='inner')\n","df = df.join(y_train,how='inner')\n","df"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
